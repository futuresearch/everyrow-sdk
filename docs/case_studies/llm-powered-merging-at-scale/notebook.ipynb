{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb0b6427",
   "metadata": {},
   "source": [
    "# LLM-powered Merging at Scale\n",
    "\n",
    "The everyrow `merge()` function joins two tables using LLMs, and LLM research agents, to identify matching rows at high accuracy. This notebook demonstrates how this scales to two tables of 2,246 rows. So each row gets LLM-level intelligence and research to find which of the 2,246 rows in the other table is the most likely match.\n",
    "\n",
    "Cost grows super linearly with the number of rows. At small scale (100 to 400 rows) the cost is negligible; at 2,246 x 2,246 rows, this cost $26.80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-40d38ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: install everyrow if needed and configure API key\n",
    "try:\n",
    "    import everyrow\n",
    "except ImportError:\n",
    "    %pip install everyrow\n",
    "\n",
    "import os\n",
    "if \"EVERYROW_API_KEY\" not in os.environ:\n",
    "    os.environ[\"EVERYROW_API_KEY\"] = \"your-api-key-here\"  # Get one at everyrow.io\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mkoy1995el",
   "metadata": {},
   "source": [
    "## Example: Matching 2,246 People to Personal Websites\n",
    "\n",
    "This example takes two tables: one with people's names and professional information (position, university, email), and another with a shuffled list of personal website URLs. The task is to determine which website belongs to which person.\n",
    "\n",
    "Most matches can be resolved by comparing names and emails against URL patterns. But some require web search to confirm ownership when the connection is not obvious from the data alone."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03955b64",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "my38zwvuk2n",
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport pandas as pd\nfrom everyrow import create_session\nfrom everyrow.ops import merge\n\npd.set_option(\"display.max_colwidth\", None)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5176b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Left table: 2246 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>position</th>\n",
       "      <th>university</th>\n",
       "      <th>email_address</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stefan Heimersheim</td>\n",
       "      <td>Research Scientist</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. stefan@heimersheim.eu \\n2. heimersheim@ast.cam.ac.uk \\n3. sh2061@cam.ac.uk</td>\n",
       "      <td>Apollo Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr Nikola Simidjievski</td>\n",
       "      <td>Postdoctoral Researcher</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>1. nikola.simidjievski@cl.cam.ac.uk</td>\n",
       "      <td>Artificial Intelligence Group</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ruotong Wang</td>\n",
       "      <td>PhD Student</td>\n",
       "      <td>University of Washington</td>\n",
       "      <td>1. ruotongw@cs.washington.edu</td>\n",
       "      <td>Social Futures Lab</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                 position                university                                                                  email_address                   organization\n",
       "0      Stefan Heimersheim       Research Scientist                       NaN  1. stefan@heimersheim.eu \\n2. heimersheim@ast.cam.ac.uk \\n3. sh2061@cam.ac.uk                Apollo Research\n",
       "1  Dr Nikola Simidjievski  Postdoctoral Researcher   University of Cambridge                                            1. nikola.simidjievski@cl.cam.ac.uk  Artificial Intelligence Group\n",
       "2            Ruotong Wang              PhD Student  University of Washington                                                  1. ruotongw@cs.washington.edu             Social Futures Lab"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "left_df = pd.read_csv(\"merge_websites_input_left_2246.csv\")\n",
    "right_df = pd.read_csv(\"merge_websites_input_right_2246.csv\")\n",
    "\n",
    "print(f\"Left table: {len(left_df)} rows\")\n",
    "left_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b2c3d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right table: 2246 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>personal_website_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. https://beau-coup.github.io/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. https://nair-p.github.io/ \\n2. https://nair-p.github.io/contact/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. https://www.murtylab.com \\n2. http://ratan.mit.edu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  personal_website_url\n",
       "0                                      1. https://beau-coup.github.io/\n",
       "1  1. https://nair-p.github.io/ \\n2. https://nair-p.github.io/contact/\n",
       "2                1. https://www.murtylab.com \\n2. http://ratan.mit.edu"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Right table: {len(right_df)} rows\")\n",
    "right_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506d52e",
   "metadata": {},
   "source": [
    "## Run Merge\n",
    "\n",
    "Run the merge at increasing scales to see how it behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8cf350",
   "metadata": {},
   "outputs": [],
   "source": "for n in [100, 200, 400, 800, 1600, 2246]:\n    async with create_session(name=f\"Website Matching (n={n})\") as session:\n        print(f\"Session URL: {session.get_url()}\")\n        result = await merge(\n            session=session,\n            task=\"Match each person to their website(s).\",\n            left_table=pd.read_csv(f\"merge_websites_input_left_{n}.csv\"),\n            right_table=pd.read_csv(f\"merge_websites_input_right_{n}.csv\"),\n        )\n    print(f\"n={n}\")\n    print(\"num of matched rows:\", len(result.data))\n    print(\"-\" * 100)\n    print()"
  },
  {
   "cell_type": "markdown",
   "id": "774d421c",
   "metadata": {},
   "source": [
    "## Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7bbbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rows = [100, 200, 400, 800, 1600, 2246]\n",
    "costs = [0.000465, 0.142, 0.293, 2.32, 16.6, 26.8]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(rows, costs, \"o-\", color=\"#2563eb\", linewidth=2, markersize=8)\n",
    "for x, y in zip(rows, costs):\n",
    "    ax.annotate(f\"${y:.2f}\", (x, y), textcoords=\"offset points\", xytext=(0, 12), ha=\"center\", fontsize=9)\n",
    "ax.set_xlabel(\"Number of rows\")\n",
    "ax.set_ylabel(\"Cost (USD)\")\n",
    "ax.set_title(\"Merge cost vs. number of rows\")\n",
    "ax.set_xticks(rows)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4297b",
   "metadata": {},
   "source": [
    "Cost grows super linearly with the number of rows. As the number of rows increases, each match becomes harder because the LLM has more candidates to consider, and more rows require web search to resolve ambiguity. At small scale (100 to 400 rows) the cost is negligible; at 2,246 rows it is $26.80."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f2a3b4",
   "metadata": {},
   "source": [
    "## Inspecting Results\n",
    "\n",
    "Sample matches from the n=800 run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"merge_websites_output_800.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d2e3f4",
   "metadata": {},
   "source": [
    "Most matches are resolved by the LLM alone. It can often match a person to their website by comparing names, emails, and URL patterns without any web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9124db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>email_address</th>\n",
       "      <th>personal_website_url</th>\n",
       "      <th>research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stefan Heimersheim</td>\n",
       "      <td>1. stefan@heimersheim.eu \\n2. heimersheim@ast.cam.ac.uk \\n3. sh2061@cam.ac.uk</td>\n",
       "      <td>1. https://ndaheim.github.io/ \\n2. https://ndaheim.github.io/publications/ \\n3. https://ndaheim.github.io/aboutme/</td>\n",
       "      <td>{\"personal_website_url\":\"This row was matched due to the information in both tables\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dr Nikola Simidjievski</td>\n",
       "      <td>1. nikola.simidjievski@cl.cam.ac.uk</td>\n",
       "      <td>1. https://www.cl.cam.ac.uk/~btd26/</td>\n",
       "      <td>{\"personal_website_url\":\"This row was matched due to the information in both tables\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                                                                  email_address                                                                                                personal_website_url                                                                           research\n",
       "0      Stefan Heimersheim  1. stefan@heimersheim.eu \\n2. heimersheim@ast.cam.ac.uk \\n3. sh2061@cam.ac.uk  1. https://ndaheim.github.io/ \\n2. https://ndaheim.github.io/publications/ \\n3. https://ndaheim.github.io/aboutme/  {\"personal_website_url\":\"This row was matched due to the information in both tables\"}\n",
       "1  Dr Nikola Simidjievski                                            1. nikola.simidjievski@cl.cam.ac.uk                                                                                 1. https://www.cl.cam.ac.uk/~btd26/  {\"personal_website_url\":\"This row was matched due to the information in both tables\"}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llm_matches = results_df[results_df[\"research\"].str.contains(\"information in both tables\", na=False)]\n",
    "llm_matches[[\"name\", \"email_address\", \"personal_website_url\", \"research\"]].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g1h2i3j4",
   "metadata": {},
   "source": [
    "For harder cases where the LLM cannot confidently match from the table data alone, everyrow automatically falls back to web search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412929b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>organization</th>\n",
       "      <th>personal_website_url</th>\n",
       "      <th>research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Charles London</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1. https://le-big-mac.github.io/</td>\n",
       "      <td>{\"personal_website_url\":\"This row was matched due to the following information found in the web:\\n\\nBased on the provided information for Charles London (PhD Student at the University of Oxford, Department of Computer Science), the following personal website URL and identifiers were found to assist in matching:\\n\\n- **Personal Website URL:** https://le-big-mac.github.io/\\n- **Official University Profile:** https://www.cs.ox.ac.uk/people/charles.london/\\n- **Google Scholar Profile:** https://scholar.google.com/citations?user=ghU-4hUAAAAJ\\n- **LinkedIn Profile:** https://uk.linkedin.com/in/charles-london\\n- **GitHub Username:** le-big-mac (associated with the personal website)\\n\\nThe entity is confirmed as a DPhil (PhD) student in the Artificial Intelligence and Machine Learning research theme at the University of Oxford, supervised by Prof. Varun Kanade. His research focuses on machine learning theory, LLMs, and reinforcement learning.\"}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              name organization              personal_website_url  research\n",
       "10  Charles London          NaN  1. https://le-big-mac.github.io/  ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "web_matches = results_df[results_df[\"research\"].str.contains(\"information found in the web\", na=False)]\n",
    "web_matches[[\"name\", \"organization\", \"personal_website_url\", \"research\"]].head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4",
   "metadata": {},
   "source": [
    "In this case, there is no obvious connection between \"Charles London\" and `le-big-mac.github.io` from the table data alone. everyrow searched the web, found his Oxford profile and GitHub username, and confirmed the match."
   ]
  }
 ],
 "metadata": {
  "everyrow": {
   "description": "Python notebook using LLM-powered merge to match 2,246 people to personal websites. Demonstrates semantic joining at scale with web search fallback."
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}